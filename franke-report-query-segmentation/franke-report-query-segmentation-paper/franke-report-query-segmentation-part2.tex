\section{connexity score segmentation} \label{approach1}

Formulating a query is one way for a user to communicate their needs to the underlying retrieval system. Sometimes a user lacks the ability to properly describe their information need in such a way that relevant documents can be effectively found. In other instances, multiword queries\footnote{queries with more than one word (or ``token'')} can contain n-grams\footnote{a group of tokens} that have more then one single meaning.\footnote{e.g.: ``chicken ready to eat''} Ambiguous n-grams make it challenging for the retrieval system to know which meaning corresponds with the current user's information need. This can retrieve more non-relevant documents.

Query segmentation is part of query rewriting. Query rewriting is a broader term for techniques that change a user's original query to improve the accuracy of the results. Some other examples are named entity recognition, stopword removal and stemming. Named entity recognition identifies entities like names and places, stopword removal eliminates non-essential query terms, and stemming reduces words to their corresponding base form, respectively.

\citet{Risvik:2003} state that early algorithms for many query rewriting techniques start with a so called ``leftmost'' approach. A leftmost approach iterates over the query from left to right and applies a list of changes if possible. This works well for most trivial queries\footnote{e.g.: one-word queries or queries only consisting of named entities} but struggles with complex multiword queries. As an example for a leftmost approach in query segmentation, the query \texttt{new jersey messi} will be segmented in \texttt{new jersey} and \texttt{messi}. The segmentation \texttt{new} and \texttt{jersey messi} is not possible. A leftmost segmentation algorithm will select the first token from the left (\texttt{new}) to form the longest possible meaningful segment to the right, which is \texttt{new jersey} in this case. After that, the leftmost algorithm selects the next token that is not already in a segmentation and repeats the process. While the semantic of \texttt{new jersey} and \texttt{messi} retrieves documents where Messi visited New Jersey, the segmentation \texttt{new} and \texttt{jersey messi} describes an interest in new football jerseys from Messi. These concepts lead to very different information needs.

To tackle this, \citeauthor{Risvik:2003} introduce a score called \textit{connexity}. Their approach uses query logs and score computation to create segments with the goal of improving retrieval accuracy for relevant documents. As the name suggests, query logs are collections of real world user queries. \citeauthor{Risvik:2003} based their approach heavily on the Alltheweb\footnote{The authors worked at \textit{FAST Search \& Transfer} in 2003, which was the company behind the search engine \url{alltheweb.com}} query logs. They tried to identify ``meaningful phrases'' in multiword queries, where a meaningful phrase is defined as an ordered list $S = \{w_1,w_2,...,w_n\}$ of words $w_i$ that satisfies two characteristics.\footnote{for $n \le 4$ as explained later}
\begin{enumerate}
\item[1.] $S$ appears frequently enough within the logs and
\item[2.] $S$ has a high mutual information in the query logs, indicating a strong relationship between the different words in the sequence.
\end{enumerate}

Connexity, the measurement \citeauthor{Risvik:2003} introduce, is
\begin{align*}
Conn(S) &= Freq(S) \cdot MI(w_1...w_{n-1},w_2...w_n),
\end{align*}
where $Freq(S)$ is the occurence frequency of sequence $S$ in the query logs, and $MI$ is the mutual information between parts of $S$ that appear next to each other in the query logs.\footnote{The derivation and origin of mutual information is not explained in the original paper by \citeauthor{Risvik:2003}, but in the second approach by \citeauthor{Bergsma:2007}}
In general, a high connexity score implies n-grams that appear often together in the same context.

\input{table-token-segments-risvik-2003}

The query logs called $L$ had roughly 600 million entries, that got reduced to about 105 million normalized entries. These entries contained queries with up to six tokens. \citeauthor{Risvik:2003} reduced the log entries again to only include unique n-grams for $n \le 4$, as seen in Table \ref{table-token-segments-risvik-2003}.
``New'' describes segments that were not seen before in any other entry.
This list, called $|L_{1234}|$, had about $144$ million entries.

The resulting list was still to large, so \citeauthor{Risvik:2003} reduced the dataset even further by applying two filter rules:
\begin{enumerate}
\item[1.] $Freq(S) \ge 5$, with $S \in |L_{1234}|$ and
\item[2.] $Conn(S) > t$, with fixed threshold $t$.\footnote{The paper contains no examples of used thresholds. The only source of example connexity values can be found in Table \ref{table-connexity-score-risvik-2003}}
\end{enumerate}

The final dataset $|L'_{1234}|$ contained about $12.5$ million entries. 
To minimze the delay for the user at query time, the segments and connexity scores were precomputed and stored in a database, so that up to 25,000 segments per second can be processed.

Another considered improvement to the described approach by \citeauthor{Risvik:2003} was using mutual information derived from document content rather than query logs, but the approach was not pursued further in the paper.

After the precomputation of the connexity values, the segmentation process itself starts at query time. The proposed algorithm by \citeauthor{Risvik:2003} computes the segmentation scores of all possible segmentations for a given query.
A segmentation score is defined as the sum of all connexity scores of segments with 2 or more tokens in the query. The connexity scores of single word segments are ignored. After the calculation of the segmentation scores for all possible segmentations, the scores are ranked. In general, higher scores indicate segmentations with greater coherence. 

In their paper, \citeauthor{Risvik:2003} show this process for the query \texttt{msdn library visual studio}. Their results are shown in Table \ref{table-connexity-score-risvik-2003}.
The segmentations are ranked from highest to lowest score. The bottom entry consist of only one word segments, so the segmentation score is 0. The next line features a three word segment \texttt{library visual studio} that has a connexity score of 7, while the other one word segment \texttt{msdn} with connexity score of 47,658 is ignored as stated above. This results in an overall segmentation score of 7.
The top entry consists of the two word segments \texttt{msdn library} with connexity score 5,110 and \texttt{visual studio} with connexity score 29,149. This query segmentation results in a combined segmentation score of 34,259. This is the highest possible score for this query and thus will be the actual segmentation of the query.

\input{table-connexity-score-risvik-2003}

One side effect of the connexity score segmentation is the feedback loop. \citeauthor{Risvik:2003} establish that an improvement in query segmentation should lead to better retrieval accuracy and thus more relevant documents.\footnote{This feedback loop only works, if the generated segmentations actually do improve the retrieval accuracy. If a generated segmentation is ``bad'', then this feedback loop will not work. One possible approach to guarantee that a segmentation is considered ``good'' is to set a segmentation score threshold. All scores over the threshold have a sufficient segmentation score to be considered ``good'' and will be used in the feedback loop, all scores below the threshold will be ignored.} Every query using the connexity segmentation can be saved as a new query log. The new query logs with improved retrieval accuracy and more relevant results can then be added to the existing query logs. When the query logs are used to update the connexity scores, the enhanced logs also optimise the query segmentation again. This creates a feedback loop that allows the system to iteratively optimise the segmentation process over time.

Lastly, \citeauthor{Risvik:2003} argue that most multiword queries can be fully segmented without isolated single-word token and discuss the relation between connexity and \textit{proximity}.\footnote{how close two or more tokens appear within a query} In their eyes, higher connexity scores imply a penalty for token spread in a document. In summary, tokens which appear in close proximity tend to also have a high connexity and are favored in search results.