\section{segmentation classification using support vector models} \label{approach2}

After the discussion on the early segmentation approach using connexity scores defined by \citeauthor{Risvik:2003}, this chapter is used to introduce yet another approach for query segmentation by \citeauthor{Bergsma:2007}. Similar to \citeauthor{Risvik:2003}, \citeauthor{Bergsma:2007} start with the definition of query interpretation in general. 

When a query, defined as a sequence of tokens, is put into a search engine, it undergoes the established process of interpretation and document retrieval, often resulting in documents relevant to the topics but containing additional unrelated words. The user input is often expressed in natural language. Because of that, the query tokens do not stand alone but are connected through syntactic and semantic relationships. Modern search engines take this characteristic also into account. For any given query the relevant documents and the order in that they are represented will change depending on the order of the input query tokens. The user therefore can make active use of query segmentation while searching the web. The most popular method is the usage of quotation marks to force search engines to prioritize a given segmentation. This way, instead of letting the retrieval system guess which segmentation is correct, the user can provide the ground truth within the query. In reality, the majority of the users do not use this feature, making query segmentation an important part of query understanding in general. 

Applications of segmentation include faster retrieval of relevant documents and filtering out irrelevant results for the actual query. Additionally, there are advanced techniques like query substitution and query expansion that can help improve retrieval effectiveness. These techniques are very important for query understanding in general but not in scope for this paper. 

The process of query segmentation by \citeauthor{Bergsma:2007} is implemented using a data-driven, supervised machine learning model, where the segmentation itself is treated as a classification task for each potential breakpoint in the token sequence. This is a new and more modern approach in comparison to the connexity score computation by \citet{Risvik:2003}.

The discussed approach by \citet{Risvik:2003}, the research on noun compound (NC) bracketing by \citet{Nakov:2005}, and the feature analysis of \citet{Keller:2003} were some of the main contributions to \citeauthor{Bergsma:2007}'s segmentation system. \citeauthor{Keller:2003} were one of the first to introduce web-derived features as an alternative to corpus-based features to improve effectiveness. \citeauthor{Nakov:2005} on the other hand suggested to look "beyond the n-gram" for co-occurrence counts (like "$w$'s $x$" or "$wx$" in text for words $w$ and $x$).
\citeauthor{Nicholson:2005} additionally added lexical pattern features involving determiners (like "the $w$ $x$" for words $w$ and $x$). Aside from these syntacitc relationships, modern segmentation models also integrate semantics in their NC interpretation process, improving effectiveness even further \cite{Girju:2005}.\\
\\
Similiar to the approach in chapter 2, \citeauthor{Bergsma:2007} define a segmentation as a mapping $S: x \to y$, where $x$ is a query of $N$ tokens with segmentation $y$. The segmentation is selected from $Y_N$, the set of all possible segmentations with cardinality $|Y_N| = 2^{N-1}$. They then use supervised machine learning to train a model to guess the optimal segmentation $\hat{y}$. The input for the model is a list $T = \{(x_0, y_0), ..., (x_i, y_i)\}$ where $x_i$ is a query and $y_i$ their best possible segmentation. For every pair exist a set of features $\Psi(x,y)$ that characterizes the segmentation for the corresponding query. The machine learning model then tries to maximize the score:
\begin{align*}
	\hat{y} &= argmax_y(Score_w(x,y)),
\end{align*}
where the score for a weighting $w$ is defined as $Score_w(x,y) = w \cdot \Psi(x,y)$. \citeauthor{Bergsma:2007} implement this using Support Vector Machines (SVMs) - a machine learning model that was first introduced by \citet{Joachims:2002}. While alternatives like Hidden Markov\footnote{\todo{short description Hidden Markov model}} models were tested, traditional SVMs had better effectiveness in comparison. The segmentation pipeline then evaluates the $N-1$ potential breakpoints for every query of length $N$, based on the token feature-set.\\
\\
As before mentioned, the SVM is trained for every query $x$ and corresponding ground truth $y$ with a set of features $\Psi(x,y)$, where every breakpoint is a seperate classification task. To describe a classification task where the potential breakpoint lays between the words $x_{L0}$ and $X_{R0}$, they use the following terminology:
\begin{align*}
\{..., X_{L2}, X_{L1}, X_{L0}, X_{R0}, X_{R1}, X_{R2},...\},
\end{align*}
with $L,R$ describing the position and distance of the word relative to the breakpoint.
The naive implementation for this task would only consider the features for the words left and right from the breakpoint. \citeauthor{Bergsma:2007} improved this version to include features for all words within a given window. This window is called \textit{decision boundary}. The size of these boundaries was set empiricistically to three.
They also differentiate between decision boundary and context features. While decision boundary features only include features for the corresponding left and right tokens of a breakpoint, context features take all tokens of the window into account.

\input{table-indicator-features-bergsma-2007}

Decision boundary features are again split up into 2 groups, indicator features and statistical features. As seen in Table \ref{table-indicator-features-bergsma-2007}, the authors used Part-of-Speech (POS) tagging as well as the position of the token as indicator features. Some unusual features are the \textit{is-free} and \textit{is-the} features. The authors included these two features because in many cases the words "free" and "the" introduce or finish segments and therefore improve segmentation accuracy.

\input{table-statistical-features-bergsma-2007}

Statistical features are shown in Table \ref{table-statistical-features-bergsma-2007} and use mutual information (MI)\footnote{\todo{short explaination mutual information}} similar to \citeauthor{Risvik:2003}. They additionally add some background to the computation of the MI score that lacks the original paper. For $x_{L0},x_{R0}$, the $MI$-value computes with
\begin{align*}
MI &= \log \frac{Pr(x_{L0}x_{R0})}{Pr(x_{L0}) \cdot Pr(x_{R0})} \\
&\Leftrightarrow \log C(x_{L0}x_{R0}) + \log K - \log C(x_{L0}) - \log C(x_{R0}).
\end{align*}
As seen in the equation, instead of the usual proximity metric, they adapted to metrics that are common in information retrieval. The mutual information for two token $x_{L0},x_{R0}$ depends on the number of all documents $K$ in the corpora and the number of documents $C$ where certain token appear.
Other statistical features include web-counts of single token or token pairs.

In addition to the described decision boundary features, context features also consider token $x_{L1}, x_{L2}$ and $x_{R1}, x_{R2}$. As the name suggests, the main advantage of context features is the use of the surrounding segmentation decisions. Examples for context features are web- and query-counts for tri- and four-grams.

In linguistic, there is always the possibility that one word impacts another word that is not it's neighbor. The authors use the example "female bus driver", where "female" references the "driver" rather than the "bus" and thous should be included in the segment. To cover these cases, pairwise counts for $x_{L0},x_{R1}$ and $x_{L1},x_{R0}$ were included as so called "dependency features". Experiments with bigger ranges where conducted, but did not improve effectiveness.\\
\\
\citeauthor{Bergsma:2007} used a subset of the AOL Search Query Log \cite{Pass:2006} for their research. An entry of the corpus contains the query, an anonymous user-ID\footnote{experiments using the AOL Search Query Log have shown that the user-IDs are not as anonymous as they appear to be. In some experiments, it was possible to trace the informations gained through the queries back to an existing person.} and URL if the person clicked on any website after searching for the corresponding query.
The authors then filtered the queries to only include entries where user clicks are present, have a query length of at least four, and contain adjectives in the query itself.
\citeauthor{Bergsma:2007} used the classic train-, validation-, and test-sets with 500 queries each. For every entry they removed the punctuation and added POS-tags. For supervised learning some kind of ground truth or gold standard is neccesary, so they manually annotated every entry in the database. They used one annotator for the training- and validation-set and two additionally annotators for the test-set. The given task for the annotators was to derive an information need based on the infos given and segment the query according to this information need. They then compared the agreement of the annotators using the Kappa score $\kappa$. For single breakpoint segmentation they achieved $\kappa \approx 84.3\%$ and for complete query segmentation $\kappa \approx 59.2\%$\footnote{\todo{maybe describe kappa statistics here}}, resulting in overall worse agreement than initially expected by the authors.
To stay consistent with the metrics, they defined single breakpoint segmentation as \textit{Seg-Acc} and complete query segmentation as \textit{Qry-Acc}. Additionally, they used McNemar's test\footnote{similar to student's t-test but commonly used for classification tasks} with $p<0.05$ to assess significance.

\input{table-segmentation-performance-bergsma-2007}

To compare the accuracy, the authors defined three baselines:\footnote{\todo{this should probably be a table for better readability}}
\begin{enumerate}
\item[1)] "always split": 44.8\% Seg-Acc, 4.2\% Qry-Acc.
\item[2)] "never split": 55.2\% Seg-Acc, 4.0\% Qry-Acc.
\item[3)] State-of-the-Art (MI): 68\% Seg-Acc, 26.6\% Qry-Acc.
\end{enumerate}
While the first two baselines were naive approaches that e.g. always split at any breakpoint, the third baseline resembled the approach by \citeauthor{Risvik:2003} that was covered earlier. This baseline can also found in Table \ref{table-segmentation-performance-bergsma-2007}. The results show, that especially the use of context features improved the accuracy significant. While the dependecy features most of the time increased the accuracy too, they in some cases also decreased Qry-Acc.