\section{segmentation classification using support vector models} \label{approach2}

This chapter is used to introduce the approach for query segmentation by \citeauthor{Bergsma:2007}. Similar to \citeauthor{Risvik:2003}, \citeauthor{Bergsma:2007} start with the definition of query interpretation in general. 

The process of query segmentation by \citeauthor{Bergsma:2007} is implemented using a supervised machine learning model, where the segmentation itself is treated as a classification task for each potential breakpoint in the query. This is a different approach in comparison to the connexity score computation by \citeauthor{Risvik:2003}.

The connexity approach by \citet{Risvik:2003}, the research on noun compound bracketing by \citet{Nakov:2005}, and the feature analysis of \citet{Keller:2003} were some of the main sources used by \citeauthor{Bergsma:2007} to create their segmentation approach. \citeauthor{Keller:2003} were one of the first to introduce web-derived\footnote{e.g.: clicks or dwell-times} features as an alternative to corpus-based features\footnote{e.g.: term frequency} to improve retrieval effectiveness. \citeauthor{Nakov:2005} on the other hand suggested to look for co-occurrence counts.\footnote{``$w$'s $x$'' or ``$wx$'' in text for words $w$ and $x$}
Another source for \citeauthor{Bergsma:2007} was a paper by \citet{Nicholson:2005}. \citeauthor{Nicholson:2005} added lexical pattern features involving determiners\footnote{``the $w$ $x$'' for words $w$ and $x$)}. Aside from these syntactic relationships, segmentation approaches also integrate semantics in their noun compound interpretation process. The noun compound interpretation process was described by \citet{Girju:2005} and used by \citeauthor{Bergsma:2007} for their segmentation approach.\\

Similiar to the approach in Chapter 2, \citeauthor{Bergsma:2007} define a segmentation as a mapping $x \to y$, where $x$ is a query of $N$ tokens with segmentation $y$. The segmentation is selected from a set $Y_N$, that contains all possible segmentations and has $2^{N-1}$ elements. \citeauthor{Bergsma:2007} then use supervised machine learning to train a model to guess the segmentation $\hat{y}$ that improves the retrieval effectiveness the most. The input for the model is a list $T = \{(x_0, y_0), ..., (x_i, y_i)\}$, where $x_i$ is a query and $y_i$ the segmentation acting as ground truth, as described later. Every pair has a set of features $\Psi(x,y)$ for this segmentation of the corresponding query. The model then computes the score:
\begin{align*}
	\hat{y} &= argmax_y(Score_w(x,y)),
\end{align*}
where the score is defined as $Score_w(x,y) = w \cdot \Psi(x,y)$. \citeauthor{Bergsma:2007} implement this model using support vector machines (SVMs), a machine learning model that was first introduced by \citet{Joachims:2002}. While alternatives like hidden Markov models were tested by \citeauthor{Bergsma:2007} for query segmentation, traditional SVMs generated better query segmentations in comparison.\\

The segmentation pipeline computes the $N-1$ potential breakpoints for a query of length $N$, based on the feature set. As before mentioned, the SVM is trained on a training set of queries $x$ and corresponding ground truth $y$ with a set of features $\Psi(x,y)$, where every breakpoint is a seperate classification task and the potential breakpoint lays between the words $X_{L0}$ and $X_{R0}$. The terminology used by \citeauthor{Bergsma:2007} describes $X_{L0}$ as the token to the left of the breakpoint and $X_{R0}$ as the token to the right of the breakpoint. A query then looks like this:
\begin{align*}
..., X_{L2}, X_{L1}, X_{L0}, X_{R0}, X_{R1}, X_{R2},...,
\end{align*}
with $L,R$ describing the position and offset of the word relative to the breakpoint. The naive implementation of this task would only consider the features for $X_{L0}$ and $X_{R0}$. \citeauthor{Bergsma:2007} included features for all words within a given window. This window is called \textit{decision boundary}. The size of these boundaries was set empirically to 3.
\citeauthor{Bergsma:2007} also differentiate between decision boundary and context features. While decision boundary features only include features for $X_{L0}$ and $X_{R0}$, context features use all tokens of the window.

\input{table-indicator-features-bergsma-2007}

Decision boundary features are again split up into two groups, ``indicator features'' and ``statistical features''. Indicator features are shown in Table \ref{table-indicator-features-bergsma-2007} and describe features of the token in context of the whole query. Examples for indicator features are part-of-speech tagging as well as the position of the token in the query. Other indicator features are \textit{is-free} and \textit{is-the}. \citeauthor{Bergsma:2007} included these two features because in many cases the words ``free'' and ``the'' introduce or end segments.

\input{table-statistical-features-bergsma-2007}

Statistical features are shown in Table \ref{table-statistical-features-bergsma-2007} and use mutual information (MI) similar to \citeauthor{Risvik:2003}. \citeauthor{Bergsma:2007} explain the computation of the MI score that was absend in the paper by \citeauthor{Risvik:2003}.
Instead of the usual proximity metric $Pr(x)$ used to compute mutual information, \citeauthor{Bergsma:2007} adapted to metrics that are common in information retrieval like document frequency. The mutual information for two token $X_{L0},X_{R0}$ depends on the number of all documents $K$ in a corpora and the document frequency $C$ where a token appears. 
For two tokens, in this example called $X_{L0}$ and $X_{R0}$, the $MI$-value computes with
\begin{align*}
MI &= \log \frac{Pr(X_{L0}X_{R0})}{Pr(X_{L0}) \cdot Pr(X_{R0})}
\end{align*}
which is equivalent to
\begin{align*}
MI &=\log C(X_{L0}X_{R0}) + \log K - \log C(X_{L0}) - \log C(X_{R0}).
\end{align*}

Other statistical features include web-counts of single token or token pairs. In addition to the described decision boundary features, context features also consider tokens $X_{L1}, X_{L2}$ and $X_{R1}, X_{R2}$. Examples for context features are web and query counts for tri- and four-grams.

In linguistics, there is always the possibility that one word impacts another word that is not it's neighbor. \citeauthor{Bergsma:2007} use the example \texttt{female bus driver}, where \texttt{female} references the \texttt{driver} rather than the \texttt{bus} and thus should be included in a potential segment. To cover these cases, pairwise frequency counts for $X_{L0},X_{R1}$ and $X_{L1},X_{R0}$ were included as so called ``dependency features''. Experiments with bigger ranges were conducted, but did not improve segmentation effectiveness.\\

\input{table-baselines-bergsma-2007}

\input{table-segmentation-performance-bergsma-2007}

\citeauthor{Bergsma:2007} used a subset of the AOL query log \cite{Pass:2006} for their research. A query log entry contains the query, an anonymous user ID\footnote{experiments using the AOL Search Query Log have shown that the user IDs are not as anonymous as they appear to be. In some experiments, it was possible to trace the informations gained through the queries back to an existing person.} and URL of the domain if the person clicked on any link.
\citeauthor{Bergsma:2007} then filtered the queries to only include entries where user clicks were present, that had a query length of at least four token, and contained adjectives in the query itself.
\citeauthor{Bergsma:2007} used train, validation, and test sets with 500 queries each. Punctuation was removed and POS-tags were added for all entries. For supervised learning, some kind of ground truth or gold standard is necessary, so the entries in the query logs needed to be manually annotated. \citeauthor{Bergsma:2007} had one annotator for the training and validation set and two additional annotators for the test set. The given task for the annotators was to formulate an information need based on the entry and segment the query according to this information need. \citeauthor{Bergsma:2007} then compared the agreement of the annotators using the Kappa score $\kappa$. For single breakpoint segmentation\footnote{The proportion of times the decision to insert a segment break between two tokens agrees with the ground truth} the annotators achieved $\kappa \approx 84.3\%$ and for complete query segmentation\footnote{The proportion of queries for which the segmentation from the SVM agrees with the ground truth} $\kappa \approx 59.2\%$, resulting in overall worse agreement than initially expected by \citeauthor{Bergsma:2007}.
They defined single breakpoint segmentation as \textit{Seg-Acc} and complete query segmentation as \textit{Qry-Acc}. Additionally, they used McNemar's test\footnote{similar to Student's t-test} with $p<0.05$ to assess significance.

To compare the accuracy, \citeauthor{Bergsma:2007} defined three baselines as seen in Table \ref{table-baselines-bergsma-2007}.
While the first two baselines were naive approaches that always or never split at any breakpoint, the third baseline used only mutual information as a metric. The score for the third baseline as well as the scores for the approach by \citeauthor{Bergsma:2007} are shown in Table \ref{table-segmentation-performance-bergsma-2007}. The results show, that especially the use of context features improved the accuracy. The dependecy features most of the time increased the accuracy, but also decreased Qry-Acc in some cases as seen in the last two rows of Table \ref{table-segmentation-performance-bergsma-2007}, where the Qry-Acc goes from 63.8\% to 61.0\% and from 71.7\% to 69.4\% respectively.